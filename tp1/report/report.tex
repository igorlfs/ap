\documentclass[a4paper]{article}

\usepackage[T1]{fontenc}              % Codificação para português 
\usepackage[brazilian]{babel}         % Português
\usepackage{hyphenat}                 % Use hifens corretamente
\usepackage{hyperref}                 % Links para páginas web
\usepackage{graphicx}                 % Figuras
\usepackage{float}                    % Figuras no lugar "esperado"
\graphicspath{{./images/}}            % Localização das imagens
\usepackage{fancyhdr}                 % Header estilo DCC
\setlength{\headheight}{24pt}

\title{Trabalho Prático I}
\author{Igor Lacerda Faria da Silva}
\date{\href{mailto:igorlfs@ufmg.br}{\texttt{igorlfs@ufmg.br}} }

\begin{document}

\pagestyle{fancy}
\lhead{Prof. Adriano Veloso

Aprendizado de Máquina}
\rhead{DCC / ICEx / UFMG 

2023.1}

\maketitle

\section{Introdução}
\label{sec:Introdução}

O trabalho consiste em montar uma rede neural para identificar dígitos escritos à mão, do banco de dados do \href{https://en.wikipedia.org/wiki/MNIST_database}{\texttt{MNIST}}. O propósito desse relatório é comparar algumas variações dessa rede neural. A rede possui apenas 3 camadas e as variações exploram diferentes algoritmos, taxas de aprendizado e número de neurônios na camada oculta.

A comparação é dividida em 3 partes: primeiro é fixado o algoritmo e a taxa de aprendizado, modificando-se o tamanho da camada oculta. Similarmente, a taxa é alterada e, por fim, o algoritmo. Os 3 algoritmos implementados (\textit{Stochastic Gradient Descent}, \textit{Gradient Descent} e \textit{Mini-Batch}) consistiram apenas em modificações de parâmetros simples da biblioteca.

A biblioteca utilizada foi o \href{https://www.tensorflow.org/}{\texttt{TensorFlow}}. Ela permite carregar o banco de dados do MNIST diretamente e possui uma \href{https://keras.io/api/}{\texttt{API}} conveniente para criar os modelos.

\section{Desenvolvimento}%
\label{sec:Desenvolvimento}

Foram analisadas duas métricas: a acurácia e a perda.

\subsection{Tamanho da Camada Oculta}%
\label{sub:Tamanho da Camada Oculta}

Foi fixada uma taxa de aprendizado $l_R = 1$ e um algoritmo de \textit{Mini-Batch}, com o tamanho da \textit{batch} igual a 50.

\begin{figure}[H]
  \begin{center}
  {\includegraphics[height=7cm]{./images/Accuracy_over_Epoch_var_Hidden.png}}
  \end{center}
  \caption{Acurácia por época, variando o tamanho da rede. \label{fig:aeh}}
\end{figure}

\begin{figure}[H]
  \begin{center}
  {\includegraphics[height=7cm]{./images/Loss_over_Epoch_var_Hidden.png}}
  \end{center}
  \caption{Perda por época, variando o tamanho da rede. \label{fig:leh}}
\end{figure}

Como é de se esperar, o aumento do tamanho da rede melhorou a performance. Existem execuções em que não é o caso, apesar de ser uma tendência comum. Com mais neurônios é possível fazer ajustes mais sensíveis aos dados. Ao que parece, no entanto, com apenas 100 neurônios ainda não há \textit{overfitting}.

\subsection{Taxa de Aprendizado}%
\label{sub:Taxa de Aprendizado}

Foi fixada um tamanho de camada oculta igual a 50, e o algoritmo de \textit{Mini-Batch}, também com tamanho de \textit{batch} igual a 50.

\begin{figure}[H]
  \begin{center}
  {\includegraphics[height=7cm]{./images/Accuracy_over_Epoch_var_Rate.png}}
  \end{center}
  \caption{Acurácia por época, variando a taxa de aprendizado. \label{fig:aer}}
\end{figure}

\begin{figure}[H]
  \begin{center}
  {\includegraphics[height=7cm]{./images/Loss_over_Epoch_var_Rate.png}}
  \end{center}
  \caption{Perda por época, variando a taxa de aprendizado. \label{fig:ler}}
\end{figure}

Como visto em aula, uma taxa de aprendizado muito alta pode prejudicar a qualidade do modelo. Isso acontece porque a função de perda “pula” possíveis mínimos locais. Apesar de ter convergido nessa execução, o desempenho para a taxa igual a 10 não é diferente de um modelo que chuta que todos os dígitos são o número 9. Por outro lado, as taxas de 0.5 e 1, para esses parâmetros e, principalmente, essa execução, foram muito próximos. Uma taxa de aprendizado muito baixa também poderia demorar a convergir. O ideal é encontrar um meio termo, que nesse caso parece ser um número próximo de 0.5 e 1.

\subsection{Algoritmo}%
\label{sub:Algoritmo}

As variações de algoritmo foram controladas alterando-se o parâmetro \textit{batch\_size}, da função \textit{fit()} do modelo. Em princípio, todos os algoritmos são o \textit{Mini-Batch}. Para criar um SGD, basta usar uma \textit{batch} de tamanho 1 e para criar um GD, é preciso usar uma taxa de aprendizado igual ao número de dados de treino. O número de épocas também precisou ser ajeitado: é preciso dar mais oportunidades de ajustes no GD, por exemplo (e o tempo de execução do SGD é muito grande). Nos gráficos, o eixo $x$ representa o “progresso” do algoritmo, cada ponto corresponde a uma época. A taxa de aprendizado foi fixada em 1, e o número de neurônios fixado em 50.

\begin{figure}[H]
  \begin{center}
  {\includegraphics[height=7cm]{./images/Loss_over_Epoch_Progress.png}}
  \end{center}
  \caption{Acurácia por progresso em épocas, variando o algoritmo. \label{fig:lep}}
\end{figure}

\begin{figure}[H]
  \begin{center}
  {\includegraphics[height=7cm]{./images/Accuracy_over_Epoch_Progress.png}}
  \end{center}
  \caption{Acurácia por progresso em épocas, variando o algoritmo.\label{fig:aep}}
\end{figure}

O SGD teve um péssimo desempenho, também indiferente a apenas chutar algum número em particular. Como são feitos ajustes a cada dado, é como se o SGD “decorasse” o dado em questão. O GD, com 100 épocas, teve um desempenho similar ao MB (\textit{batch\_size = 50}). Com a diminuição do tamanho da \textit{batch}, o desempenho piorou consideravelmente.

\end{document}
